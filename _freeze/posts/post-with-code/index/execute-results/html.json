{
  "hash": "31b3e76ac676f8a9e9cd4d1f1db20e5b",
  "result": {
    "markdown": "---\ntitle: \"Titanic from Kaggle\"\nauthor: \"Stephen Parton\"\ndate: \"2022-09-01\"\ncategories: [code, analysis,titanic]\nwebsite:\n  sidebar:\n    style: \"docked\"\n    search: true\n    contents:\n      - section: \"Data Exploration\"\n      - index.qmd\nformat: \n  html: \n    theme: litera\n    toc: true\n    toc-title: Contents\n    number-sections: true\n    number-depth: 3\n    code-fold: true\n    code-summary: \"Code\"\n    code-tools: true\nexecute: \n  echo: true\n  warning: false\n  error: false\n  freeze: true\n  cache: true\n---\n\n\n![](thumbnail.jpg){width=\"215\"}\n\n## Summary\n\nThis is just a first test with code in a blog using the new Quarto framework! Guess what I am using..yep Titanic, Kaggle version..\n\nIt is not very well structured as it is pretty much in the order I did it following all instructions, books and blogs from the expert TidyModels and Quarto teams at RStudio/Posit . All errors belong to me!\n\n\n\n\n\n## Final Kaggle Scores\n\n\n::: {.cell hash='index_cache/html/kaggle_984879db7be90afea28d160810ca1102'}\n\n```{.r .cell-code}\nkaggle <- tibble(\n  Model = c(\"Logistic Regression\",\n            \"Regularised Logistic Regression\",\n            \"Random Forest-final\",\n            \"Random Forest-initial\",\n            \"XG Boost\",\n            \"Neural Net\",\n            \"Ensemble\"), \n  Score = c(.76555,.77033,.77751,.78229,.77272,.76794,.77751)\n  )\n\nkaggle %>% knitr::kable()\n```\n\n::: {.cell-output-display}\n|Model                           |   Score|\n|:-------------------------------|-------:|\n|Logistic Regression             | 0.76555|\n|Regularised Logistic Regression | 0.77033|\n|Random Forest-final             | 0.77751|\n|Random Forest-initial           | 0.78229|\n|XG Boost                        | 0.77272|\n|Neural Net                      | 0.76794|\n|Ensemble                        | 0.77751|\n:::\n:::\n\n\nWhich when all submitted gave me a ranking of 1,872 out of 13,000 or so teams, so no grand-master!\n\nSeems like the value mainly comes from the feature engineering and selection process (as the experts all seem to say) given the similarity in above model scores.\n\n## Review Data\n\n### Load Some Kaggle Data\n\nNot the...? Yes, the Titanic again....\n\n\n::: {.cell hash='index_cache/html/data_20fd518fafb705a26689b195f1e5d2f0'}\n\n```{.r .cell-code}\ntrain <- read_csv(\"data_raw/train.csv\",show_col_types = FALSE) %>% clean_names() %>% mutate(train_test = \"train\")\ntest <- read_csv(\"data_raw/test.csv\",show_col_types = FALSE) %>% clean_names() %>% \n  mutate(train_test = \"test\")\nall <- train %>% bind_rows(test)\n\n# colnames(data)\n# cwd()\n```\n:::\n\n\n### Some Initial EDA\n\nA quick look.\n\n\n::: {.cell hash='index_cache/html/skim_18827564c4a0cb0d3752f8d03e18abc2'}\n\n```{.r .cell-code}\ntrain %>% skim() \n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |Piped data |\n|Number of rows           |891        |\n|Number of columns        |13         |\n|_______________________  |           |\n|Column type frequency:   |           |\n|character                |6          |\n|numeric                  |7          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|name          |         0|          1.00|  12|  82|     0|      891|          0|\n|sex           |         0|          1.00|   4|   6|     0|        2|          0|\n|ticket        |         0|          1.00|   3|  18|     0|      681|          0|\n|cabin         |       687|          0.23|   1|  15|     0|      147|          0|\n|embarked      |         2|          1.00|   1|   1|     0|        3|          0|\n|train_test    |         0|          1.00|   5|   5|     0|        1|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|     sd|   p0|    p25|    p50|   p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|------:|----:|------:|------:|-----:|------:|:-----|\n|passenger_id  |         0|           1.0| 446.00| 257.35| 1.00| 223.50| 446.00| 668.5| 891.00|▇▇▇▇▇ |\n|survived      |         0|           1.0|   0.38|   0.49| 0.00|   0.00|   0.00|   1.0|   1.00|▇▁▁▁▅ |\n|pclass        |         0|           1.0|   2.31|   0.84| 1.00|   2.00|   3.00|   3.0|   3.00|▃▁▃▁▇ |\n|age           |       177|           0.8|  29.70|  14.53| 0.42|  20.12|  28.00|  38.0|  80.00|▂▇▅▂▁ |\n|sib_sp        |         0|           1.0|   0.52|   1.10| 0.00|   0.00|   0.00|   1.0|   8.00|▇▁▁▁▁ |\n|parch         |         0|           1.0|   0.38|   0.81| 0.00|   0.00|   0.00|   0.0|   6.00|▇▁▁▁▁ |\n|fare          |         0|           1.0|  32.20|  49.69| 0.00|   7.91|  14.45|  31.0| 512.33|▇▁▁▁▁ |\n:::\n:::\n\n\n### Some Initial Wrangling\n\n\n::: {.cell hash='index_cache/html/wrangle_1_420833d25bebed479ab3403776760b0c'}\n\n```{.r .cell-code}\nall_proc <- all %>% \n  mutate(title = str_extract(name,\"(\\\\w)([a-z]+)(\\\\.)\")) %>% \n  mutate(pax_type = case_when(\n    title %in% c(\"Miss.\",\"Ms.\",\"Mlle.\")         ~ \"F_unmarried\",\n    title %in% c(\"Mme.\",\"Mrs.\")                 ~ \"F_married\",\n    title %in% c(\"Countess.\",\"Lady.\",\"Dona.\")   ~ \"F_titled\",\n    title %in% c(\"Capt.\",\"Col.\",\"Major.\")       ~ \"Military\",\n    title %in% c(\"Dr.\",\"Rev.\")                  ~ \"M_Professional\",\n    title %in% c(\"Don.\",\"Jonkheer.\",\"Sir.\")     ~ \"M_titled\",\n    TRUE ~ title\n  ),\n  surname        = str_extract(name,\"(\\\\w+)(\\\\,)\"),\n  survival       = ifelse(survived==0,\"No\",\"Yes\"),\n  ticket_preface = str_extract(ticket,\"([:graph:]+)(\\\\s)\"),\n  ticket_preface = ifelse(is.na(ticket_preface),\"none\",ticket_preface),\n  cabin_preface  = ifelse(is.na(cabin),\"nk\",\n                    substr(cabin,1,1)),\n  embarked       = ifelse(is.na(embarked),\"S\",embarked)\n  ) %>% \n  group_by(pax_type,pclass) %>% \n  mutate(age     = ifelse(is.na(age),median(age,na.rm = T), age)) %>% \n  ungroup() %>% \n  add_count(ticket,name = \"ticket_group\") %>% \n  mutate(ticket_group = case_when(\n    ticket_group == 1 ~ \"single\",\n    ticket_group == 2 ~ \"couple\",\n    TRUE              ~ \"group\"\n  ),\n    family_group = as.numeric(sib_sp)+as.numeric(parch)+1\n  ) %>% \n  mutate(family_group = factor(\n    case_when(\n        family_group < 2  ~ \"single\",\n        family_group < 3  ~ \"couple\",\n        TRUE              ~ \"family\"\n        ),\n    ordered = TRUE)\n  ) %>% \n  mutate(age_group = factor(case_when(\n    age < 13      ~ \"child\",\n    age < 20      ~ \"teen\",\n    age < 30      ~ \"20s\",\n    age < 40      ~ \"30s\",\n    age < 50      ~ \"40s\",\n    age < 60      ~ \"50s\",\n    TRUE          ~ \"60+\"\n    \n  ),\n  ordered = TRUE)\n  ) %>% \n  mutate(across(where(is.character),as_factor)) %>% \n  mutate(pclass = factor(pclass,levels = c(\"1\",\"2\",\"3\")),\n         survived = factor(survived)\n         ) %>% \nselect(-c(title,ticket_preface))\n  \n#all_proc %>% glimpse() \n```\n:::\n\n\n### A bit more EDA\n\n\n::: {.cell hash='index_cache/html/EDA_1_abdf445b06aa5aa2f1c6d88905f30237'}\n\n```{.r .cell-code}\nall_proc %>% \n  select(-c(name,ticket,cabin,surname,train_test)) %>% \n  DataExplorer::plot_bar()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/EDA_1-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/EDA_1-2.png){width=672}\n:::\n:::\n\n::: {.cell hash='index_cache/html/data_explorer1_4d1a0b0012a8599d14abe66c8dfd223b'}\n\n```{.r .cell-code}\nall_proc %>% DataExplorer::plot_histogram(ggtheme = theme_light() )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/data_explorer1-1.png){width=672}\n:::\n:::\n\n\n### Eyeballing Survival Graphs on Training Data\n\n\n::: {.cell hash='index_cache/html/eye_ball_survival_349e12b4a38bdcea2bcbbfe25ae93751'}\n\n```{.r .cell-code}\nno_f <- all_proc %>%\n  filter(train_test == \"train\") %>% \n  select(passenger_id,pclass,sex,embarked,pax_type,ticket_group,family_group,age_group,cabin_preface,survival) %>% \n  droplevels() %>%\n  mutate(across(where(is.factor),~ factor(.x,ordered = FALSE))) %>%\n  pivot_longer(cols = c(pclass:cabin_preface)) \n\n\ng_l <- no_f %>% \n  split(.$name) %>% \n  map(~ ggplot(.,aes(y=value,fill=survival)) +\n                geom_bar() +\n              ggtitle(.$name) +\n        theme_bw() +\n        labs(x=NULL,y=NULL)+\n        scale_fill_viridis_d(option = \"cividis\")\n      \n            ) \n\nlibrary(patchwork)\nwrap_plots(g_l, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/eye_ball_survival-1.png){width=672}\n:::\n:::\n\n\n### Split Data back to Train/Test/Validation\n\n\n::: {.cell hash='index_cache/html/split_328803bb933c01dd626d419a00a3a297'}\n\n```{.r .cell-code}\ntrain_proc_adj_tbl <- all_proc %>% \n  filter(train_test ==\"train\") %>% \n  select(-c(survival))\n\n\n  \ntrain_split <- initial_split(train_proc_adj_tbl,strata = survived)\n\ntrain_train <- training(train_split)\ntrain_test <- testing(train_split)\n```\n:::\n\n\n## Recipe-Base\n\n\n::: {.cell hash='index_cache/html/recipe_base_5223881573ccfd55472e5aadbccd2f2b'}\n\n```{.r .cell-code}\nrecipe_base <- \n  recipe(survived ~ ., data = train_train) %>% \n  update_role(passenger_id, name,surname,ticket,cabin,new_role = \"ID\") %>%\n  step_impute_knn(all_numeric_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>%\n  step_factor2string(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_pca()\nrecipe_base\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        ID          5\n   outcome          1\n predictor         13\n\nOperations:\n\nK-nearest neighbor imputation for all_numeric_predictors()\nDummy variables from all_nominal_predictors()\nCharacter variables from all_nominal_predictors()\nZero variance filter on all_predictors()\nPCA extraction with <none>\n```\n:::\n:::\n\n\n### Save Files\n\n\n::: {.cell hash='index_cache/html/save_rds_880e2b7cbdfbb891c1d8f483094c1714'}\n\n```{.r .cell-code}\nwrite_rds(all_proc,\"artifacts/all_proc.rds\")\nwrite_rds(train_split,\"artifacts/train_split.rds\")\nwrite_rds(recipe_base,\"artifacts/recipe_base.rds\")\n# \n# all_proc <- read_rds(\"artifacts/all_proc.rds\")\n# train_split <- read_rds(\"artifacts/train_split.rds\")\n# recipe_base <- read_rds(\"artifacts/recipe_base.rds\")\n```\n:::\n\n\n## Models\n\n### Logistic Regression\n\n#### LR Model Spec\n\n\n::: {.cell hash='index_cache/html/LR_model_638b3a1b440d0c973b915afc4970d922'}\n\n```{.r .cell-code}\nlr_spec <-  \n  logistic_reg() %>% \n  set_engine(\"glm\")\n\nlr_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n\n#### LR Workflow\n\n\n::: {.cell hash='index_cache/html/LR_wflow_fd39665b5c220ade639f58c4ee4cdb29'}\n\n```{.r .cell-code}\nlr_wflow <- \n  workflow() %>% \n  add_model(lr_spec) %>% \n  add_recipe(recipe_base)\n\nlr_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_dummy()\n• step_factor2string()\n• step_zv()\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n\n#### LR Fit Model\n\n\n::: {.cell hash='index_cache/html/LR_fit_dafac423ac89c580c05c4eab9301e7e2'}\n\n```{.r .cell-code}\nlr_fit <- \n  lr_wflow %>% \n  last_fit(train_split)\n\n#lr_fit\n\nlr_final_metrics <- lr_fit %>% collect_metrics()\nlr_final_metrics \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.799 Preprocessor1_Model1\n2 roc_auc  binary         0.822 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n#show_notes(.Last.tune.result)\n```\n:::\n\n\n#### LR Predict\n\n\n::: {.cell hash='index_cache/html/LR_pred_91a8f697554997a89fe23f0a2f4005f9'}\n\n```{.r .cell-code}\nlr_test_predictions <- lr_fit %>% collect_predictions() %>% \n  rename(survived_pred = survived) %>% \n  bind_cols(train_test)\nlr_test_predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 224 × 26\n   id       .pred_0 .pred_1  .row .pred…¹ survi…² .config passe…³ survi…⁴ pclass\n   <chr>      <dbl>   <dbl> <int> <fct>   <fct>   <chr>     <dbl> <fct>   <fct> \n 1 train/t…  0.0948 9.05e-1    10 1       1       Prepro…      10 1       2     \n 2 train/t…  1.00   2.63e-7    11 0       1       Prepro…      11 1       3     \n 3 train/t…  0.996  4.01e-3    14 0       0       Prepro…      14 0       3     \n 4 train/t…  0.749  2.51e-1    18 0       1       Prepro…      18 1       2     \n 5 train/t…  0.122  8.78e-1    20 1       1       Prepro…      20 1       3     \n 6 train/t…  0.309  6.91e-1    23 1       1       Prepro…      23 1       3     \n 7 train/t…  0.887  1.13e-1    28 0       0       Prepro…      28 0       1     \n 8 train/t…  0.473  5.27e-1    40 1       1       Prepro…      40 1       3     \n 9 train/t…  0.468  5.32e-1    41 1       0       Prepro…      41 0       3     \n10 train/t…  0.862  1.38e-1    51 0       0       Prepro…      51 0       3     \n# … with 214 more rows, 16 more variables: name <fct>, sex <fct>, age <dbl>,\n#   sib_sp <dbl>, parch <dbl>, ticket <fct>, fare <dbl>, cabin <fct>,\n#   embarked <fct>, train_test <fct>, pax_type <fct>, surname <fct>,\n#   cabin_preface <fct>, ticket_group <fct>, family_group <ord>,\n#   age_group <ord>, and abbreviated variable names ¹​.pred_class,\n#   ²​survived_pred, ³​passenger_id, ⁴​survived\n```\n:::\n:::\n\n\n#### LR Performance on validation set\n\n##### AUC Curve\n\n\n::: {.cell hash='index_cache/html/LR_auc_aa6c575263ad88813e46b10372b7eac5'}\n\n```{.r .cell-code}\nlr_test_predictions %>% \n  roc_curve(truth = survived,.pred_1,event_level=\"second\") %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/LR_auc-1.png){width=672}\n:::\n:::\n\n\n##### Confusion Matrix\n\n\n::: {.cell hash='index_cache/html/LR_confuse_e78200675ad55dce7a5b2ed6d9d532d4'}\n\n```{.r .cell-code}\nlr_test_predictions %>% \n  conf_mat(survived,.pred_class) %>% \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/LR_confuse-1.png){width=672}\n:::\n:::\n\n\n#### LR Resampling\n\n\n::: {.cell hash='index_cache/html/LR_resample_b0d8262e72fc8d56c5010b74d13b76c1'}\n\n```{.r .cell-code}\nfolds <- vfold_cv(train_train, strata = survived, v=5)\n#folds\n\ncontrol <- control_resamples(save_pred = TRUE,save_workflow = TRUE)\n\ncores <- parallel::detectCores()\ncl <- parallel::makePSOCKcluster(cores - 1)\n\n# doParallel::registerDoParallel(cores = cores)\nset.seed(1234)\nlr_fit_cv <- \n  lr_wflow %>% \n  fit_resamples(folds, control = control)\n\n#show_best(lr_fit_cv,metric= \"accuracy\")\n\n#lr_fit_cv\nlr_metrics_resample <- collect_metrics(lr_fit_cv)\nlr_metrics_resample\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.823     5 0.0124  Preprocessor1_Model1\n2 roc_auc  binary     0.856     5 0.00839 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\nparallel::stopCluster(cl)\n```\n:::\n\n\nFollowing still to be fixed!\n\n\n::: {.cell hash='index_cache/html/LR_preds_82f2086d9d8b1691862ff1d64a9fc4fd'}\n\n```{.r .cell-code}\n#lr_param <- extract_parameter_set_dials(lr_spec)\n\nlr_resample_test_predictions <- collect_predictions(lr_fit_cv) %>% \n  rename(survived_pred = survived) \n#  bind_cols(testing(train_split))\nlr_resample_test_predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 667 × 7\n   id    .pred_0 .pred_1  .row .pred_class survived_pred .config             \n   <chr>   <dbl>   <dbl> <int> <fct>       <fct>         <chr>               \n 1 Fold1   0.888 0.112       2 0           0             Preprocessor1_Model1\n 2 Fold1   0.756 0.244      10 0           0             Preprocessor1_Model1\n 3 Fold1   0.932 0.0677     15 0           0             Preprocessor1_Model1\n 4 Fold1   0.187 0.813      20 1           0             Preprocessor1_Model1\n 5 Fold1   0.919 0.0810     26 0           0             Preprocessor1_Model1\n 6 Fold1   0.978 0.0219     29 0           0             Preprocessor1_Model1\n 7 Fold1   0.974 0.0265     34 0           0             Preprocessor1_Model1\n 8 Fold1   0.993 0.00666    36 0           0             Preprocessor1_Model1\n 9 Fold1   0.921 0.0793     43 0           0             Preprocessor1_Model1\n10 Fold1   0.924 0.0763     44 0           0             Preprocessor1_Model1\n# … with 657 more rows\n```\n:::\n:::\n\n::: {.cell hash='index_cache/html/LR_fit2_965b18889fef9944b6b60177debf16e5'}\n\n```{.r .cell-code}\ncl <- parallel::makePSOCKcluster(cores - 1)\n\nset.seed(1234)\nlm_fit <- lr_wflow %>% fit(data = train_proc_adj_tbl)\nextract_recipe(lm_fit, estimated = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        ID          5\n   outcome          1\n predictor         13\n\nTraining data contained 891 data points and 687 incomplete rows. \n\nOperations:\n\nK-nearest neighbor imputation for age, sib_sp, parch, fare [trained]\nDummy variables from pclass, sex, embarked, train_test, pax_type, cabin_prefac... [trained]\nCharacter variables from <none> [trained]\nZero variance filter removed train_test_test [trained]\nNo PCA components were extracted from <none> [trained]\n```\n:::\n\n```{.r .cell-code}\nparallel::stopCluster(cl)\n```\n:::\n\n\n## Regularised Logistic Regression - GLMNET\n\n### RLR Model Spec\n\n\n::: {.cell hash='index_cache/html/rlr_model_42b8291879cef883289b1e4e30549b58'}\n\n```{.r .cell-code}\nrlr_model <- \n  logistic_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\")\nrlr_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### RLR Parameter Tuning\n\n\n::: {.cell hash='index_cache/html/rlr_tuning_95e9702b3f1798a7adeffa6583a34c50'}\n\n```{.r .cell-code}\nrlr_param <- extract_parameter_set_dials(rlr_model)\n\nrlr_grid <- grid_latin_hypercube(\n  penalty(),\n  mixture(),\n  size = 30\n)\nhead(rlr_grid) %>% knitr::kable(digits =3)\n```\n\n::: {.cell-output-display}\n| penalty| mixture|\n|-------:|-------:|\n|   0.116|   0.866|\n|   0.095|   0.736|\n|   0.000|   0.141|\n|   0.000|   0.458|\n|   0.025|   0.983|\n|   0.484|   0.109|\n:::\n:::\n\n\n### RLR Workflow\n\n\n::: {.cell hash='index_cache/html/rlr_wflow_299fa0a8270234a43da37ff6e065caaf'}\n\n```{.r .cell-code}\nrlr_wflow <- \n  workflow() %>% \n  add_model(rlr_model) %>% \n  add_recipe(recipe_base)\nrlr_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_dummy()\n• step_factor2string()\n• step_zv()\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = tune()\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n### RLR Hyper-parameter Tuning\n\n\n::: {.cell hash='index_cache/html/rlr_cvs_d5a28d55435a1ff571c3703b5d457011'}\n\n```{.r .cell-code}\n# rlr_folds <- vfold_cv(training(train_split), strata = survived, v=10,repeats = 5)\n# rlr_folds %>% tidy()\n\n#doParallel::registerDoParallel(cores = cores)\ncl <- parallel::makePSOCKcluster(cores - 1)\n\nset.seed(234)\nrlr_tuning_result <- tune_grid(\n  rlr_wflow,\n  resamples = folds,\n  grid      = rlr_grid,\n  control   = control_grid(save_pred = TRUE, save_workflow = TRUE)\n)\n\nrlr_tuning_metrics <- collect_metrics(rlr_tuning_result)\nhead(rlr_tuning_metrics) %>% knitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n| penalty| mixture|.metric  |.estimator |  mean|  n| std_err|.config               |\n|-------:|-------:|:--------|:----------|-----:|--:|-------:|:---------------------|\n|   0.016|   0.018|accuracy |binary     | 0.821|  5|   0.016|Preprocessor1_Model01 |\n|   0.016|   0.018|roc_auc  |binary     | 0.865|  5|   0.009|Preprocessor1_Model01 |\n|   0.000|   0.066|accuracy |binary     | 0.825|  5|   0.015|Preprocessor1_Model02 |\n|   0.000|   0.066|roc_auc  |binary     | 0.857|  5|   0.008|Preprocessor1_Model02 |\n|   0.000|   0.070|accuracy |binary     | 0.825|  5|   0.015|Preprocessor1_Model03 |\n|   0.000|   0.070|roc_auc  |binary     | 0.857|  5|   0.008|Preprocessor1_Model03 |\n:::\n\n```{.r .cell-code}\nparallel::stopCluster(cl)\n```\n:::\n\n\nReview hyper-parameter tuning results and select best\n\n\n::: {.cell hash='index_cache/html/rlr_tune_ec0febfa0770d3efab5ebe41787c519a'}\n\n```{.r .cell-code}\nrlr_tuning_result %>%\n  collect_metrics() %>%\n  filter(.metric == \"accuracy\") %>%\n  select(mean, penalty,mixture) %>%\n  pivot_longer(penalty:mixture,\n               values_to = \"value\",\n               names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rlr_tune-1.png){width=672}\n:::\n\n```{.r .cell-code}\nshow_best(rlr_tuning_result, \"accuracy\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n   penalty mixture .metric  .estimator  mean     n std_err .config              \n     <dbl>   <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n1 8.86e- 3   0.923 accuracy binary     0.829     5  0.0169 Preprocessor1_Model28\n2 1.36e- 9   0.141 accuracy binary     0.826     5  0.0137 Preprocessor1_Model05\n3 1.30e-10   0.182 accuracy binary     0.826     5  0.0137 Preprocessor1_Model06\n4 5.43e- 9   0.211 accuracy binary     0.826     5  0.0137 Preprocessor1_Model07\n5 2.52e- 8   0.241 accuracy binary     0.826     5  0.0137 Preprocessor1_Model08\n```\n:::\n\n```{.r .cell-code}\nbest_rlr_auc <- select_best(rlr_tuning_result, \"accuracy\")\nbest_rlr_auc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  penalty mixture .config              \n    <dbl>   <dbl> <chr>                \n1 0.00886   0.923 Preprocessor1_Model28\n```\n:::\n:::\n\n\n### RLR Predict\n\n\n::: {.cell hash='index_cache/html/rlr_predict1_6800bdff5903c0794ca00d3bf8b2f875'}\n\n```{.r .cell-code}\nrlr_final_wflow <- finalize_workflow(\n  rlr_wflow,\n  best_rlr_auc\n)\n\nrlr_final_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_dummy()\n• step_factor2string()\n• step_zv()\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.00885574773491923\n  mixture = 0.923322001239285\n\nComputational engine: glmnet \n```\n:::\n\n```{.r .cell-code}\nrlr_final_wflow %>%\n  last_fit(train_split) %>%\n  extract_fit_parsnip() %>%\n  vip(geom = \"col\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rlr_predict1-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='index_cache/html/rlr_predict2_87c7d2e79c9fda4edd047fcd7919cec5'}\n\n```{.r .cell-code}\nrlr_final_fit <- rlr_final_wflow %>%\n  last_fit(train_split)\n\nrlr_final_metrics <- collect_metrics(rlr_final_fit)\nrlr_final_metrics %>% knitr::kable()\n```\n\n::: {.cell-output-display}\n|.metric  |.estimator | .estimate|.config              |\n|:--------|:----------|---------:|:--------------------|\n|accuracy |binary     | 0.8080357|Preprocessor1_Model1 |\n|roc_auc  |binary     | 0.8424756|Preprocessor1_Model1 |\n:::\n\n```{.r .cell-code}\nrlr_test_predictions <- rlr_final_fit %>% collect_predictions()\nrlr_test_predictions_all <- rlr_test_predictions %>% \n  bind_cols(train_test %>% select(-survived)) \n\n\n\nglimpse(rlr_test_predictions_all)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 224\nColumns: 25\n$ id            <chr> \"train/test split\", \"train/test split\", \"train/test spli…\n$ .pred_0       <dbl> 0.1216896, 0.8658490, 0.9641622, 0.7243443, 0.2225635, 0…\n$ .pred_1       <dbl> 0.87831041, 0.13415097, 0.03583775, 0.27565575, 0.777436…\n$ .row          <int> 10, 11, 14, 18, 20, 23, 28, 40, 41, 51, 54, 57, 61, 66, …\n$ .pred_class   <fct> 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,…\n$ survived      <fct> 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,…\n$ .config       <chr> \"Preprocessor1_Model1\", \"Preprocessor1_Model1\", \"Preproc…\n$ passenger_id  <dbl> 10, 11, 14, 18, 20, 23, 28, 40, 41, 51, 54, 57, 61, 66, …\n$ pclass        <fct> 2, 3, 3, 2, 3, 3, 1, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3,…\n$ name          <fct> \"Nasser, Mrs. Nicholas (Adele Achem)\", \"Sandstrom, Miss.…\n$ sex           <fct> female, female, male, male, female, female, male, female…\n$ age           <dbl> 14, 4, 39, 30, 31, 15, 19, 14, 40, 7, 29, 21, 22, 6, 21,…\n$ sib_sp        <dbl> 1, 1, 1, 0, 0, 0, 3, 1, 1, 4, 1, 0, 0, 1, 0, 0, 0, 0, 3,…\n$ parch         <dbl> 0, 1, 5, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ ticket        <fct> 237736, PP 9549, 347082, 244373, 2649, 330923, 19950, 26…\n$ fare          <dbl> 30.0708, 16.7000, 31.2750, 13.0000, 7.2250, 8.0292, 263.…\n$ cabin         <fct> NA, G6, NA, NA, NA, NA, C23 C25 C27, NA, NA, NA, NA, NA,…\n$ embarked      <fct> C, S, S, S, C, Q, S, C, S, S, S, S, C, C, S, S, Q, S, S,…\n$ train_test    <fct> train, train, train, train, train, train, train, train, …\n$ pax_type      <fct> F_married, F_unmarried, Mr., Mr., F_married, F_unmarried…\n$ surname       <fct> \"Nasser,\", \"Sandstrom,\", \"Andersson,\", \"Williams,\", \"Mas…\n$ cabin_preface <fct> nk, G, nk, nk, nk, nk, C, nk, nk, nk, nk, nk, nk, nk, nk…\n$ ticket_group  <fct> couple, group, group, single, single, single, group, cou…\n$ family_group  <ord> couple, family, family, single, single, single, family, …\n$ age_group     <ord> teen, child, 30s, 30s, 30s, teen, teen, teen, 40s, child…\n```\n:::\n\n```{.r .cell-code}\n# rlr_pred <- predict(rlr_final_fit,train_2 )%>% \n#   bind_cols(predict(rlr_final_fit, train_2,type=\"prob\")) %>% \n#   bind_cols(train_2 %>% select(survived))\n# \n# rlr_pred %>% \n#   roc_auc(truth = survived, .pred_1, event_level = \"second\")\n# \n# rlr_pred %>% \n#   roc_curve(truth = survived, .pred_1,event_level=\"second\") %>% \n#   autoplot()\n# \n# \n# rlr_metrics <- rlr_pred %>% \n# metrics(truth = survived, estimate = .pred_class) %>% \n#   filter(.metric == \"accuracy\")\n# rlr_metrics\n# survive_rlr_pred <- \n#   augment(survive_lr_fit, train_2)\n# survive_rlr_pred\n```\n:::\n\n\n### RLR Confusion Matrix\n\n\n::: {.cell hash='index_cache/html/rlr_confusion_matrix_e421a53389062ef068fa9293daf6bd1c'}\n\n```{.r .cell-code}\nrlr_test_predictions %>% conf_mat(survived,.pred_class) %>% \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rlr_confusion_matrix-1.png){width=672}\n:::\n:::\n\n\n## Random Forest\n\n### RF Model Spec - Ranger\n\n\n::: {.cell hash='index_cache/html/rf_model_1f82115a9114aaf319e25cc37928ee4f'}\n\n```{.r .cell-code}\nrf_model <- \n  rand_forest(\n    trees = 1000,\n    mtry  = tune(),\n    min_n = tune()\n    ) %>% \n  set_engine(\"ranger\",importance = \"permutation\") %>% \n  set_mode(\"classification\")\n```\n:::\n\n\n### RF Workflow\n\n\n::: {.cell hash='index_cache/html/rf_wflow_183cf1a60c159e818f8fbd136824ec9d'}\n\n```{.r .cell-code}\nrf_wflow <- \n  workflow() %>% \n  add_model(rf_model) %>% \n  add_recipe(recipe_base)\n```\n:::\n\n\n### RF Tuning - Initial\n\n\n::: {.cell hash='index_cache/html/rf_tuning_bea82b2ea60780067ef3c0fe87dc7335'}\n\n```{.r .cell-code}\ncl <- parallel::makePSOCKcluster(cores - 1)\n\nset.seed(1234)\nrf_tuning_result <- tune_grid(\n  rf_wflow,\n  resamples = folds,\n  grid = 20\n)\nparallel::stopCluster(cl)\n\nrf_tuning_result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 4\n  splits            id    .metrics          .notes          \n  <list>            <chr> <list>            <list>          \n1 <split [532/135]> Fold1 <tibble [40 × 6]> <tibble [0 × 3]>\n2 <split [534/133]> Fold2 <tibble [40 × 6]> <tibble [0 × 3]>\n3 <split [534/133]> Fold3 <tibble [40 × 6]> <tibble [0 × 3]>\n4 <split [534/133]> Fold4 <tibble [40 × 6]> <tibble [0 × 3]>\n5 <split [534/133]> Fold5 <tibble [40 × 6]> <tibble [0 × 3]>\n```\n:::\n\n```{.r .cell-code}\nrf_tuning_result %>% \n  collect_metrics() %>% \n  filter(.metric == \"accuracy\") %>% \n  select(mean,min_n,mtry) %>% \n  pivot_longer(min_n:mtry) %>% \n  ggplot(aes(value, mean, color = name)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~name, scales = \"free_x\") +\n  labs(x = NULL, y = \"Accuracy\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rf_tuning-1.png){width=672}\n:::\n:::\n\n\nBit hard to make much of it, but say min_n between 10 and 40 and mtry between 10 and 30?\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_74b8cc12c64ae19afc2d5284fb80a660'}\n\n```{.r .cell-code}\nrf_grid <- grid_regular(\n  mtry(range = c(5, 40)),\n  min_n(range = c(5, 30)),\n  levels = 5\n)\n\nrf_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 25 × 2\n    mtry min_n\n   <int> <int>\n 1     5     5\n 2    13     5\n 3    22     5\n 4    31     5\n 5    40     5\n 6     5    11\n 7    13    11\n 8    22    11\n 9    31    11\n10    40    11\n# … with 15 more rows\n```\n:::\n:::\n\n\n### RF Graph Results\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_ed31527b600c8b9f6d4570d6ea097e67'}\n\n```{.r .cell-code}\ncl <- parallel::makePSOCKcluster(cores - 1)\n\n\nset.seed(1234)\nrf_grid_tune <- tune_grid(\n  rf_wflow,\n  resamples = folds,\n  grid = rf_grid\n)\nrf_grid_tune\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 4\n  splits            id    .metrics          .notes          \n  <list>            <chr> <list>            <list>          \n1 <split [532/135]> Fold1 <tibble [50 × 6]> <tibble [5 × 3]>\n2 <split [534/133]> Fold2 <tibble [50 × 6]> <tibble [5 × 3]>\n3 <split [534/133]> Fold3 <tibble [50 × 6]> <tibble [5 × 3]>\n4 <split [534/133]> Fold4 <tibble [50 × 6]> <tibble [5 × 3]>\n5 <split [534/133]> Fold5 <tibble [50 × 6]> <tibble [5 × 3]>\n\nThere were issues with some computations:\n\n  - Warning(s) x10: 40 columns were requested but there were 33 predictors in the dat...   - Warning(s) x15: 40 columns were requested but there were 33 predictors in the dat...\n\nRun `show_notes(.Last.tune.result)` for more information.\n```\n:::\n\n```{.r .cell-code}\nparallel::stopCluster(cl)\n\nrf_grid_tune %>%\n  collect_metrics() %>%\n  filter(.metric == \"accuracy\") %>%\n  mutate(min_n = factor(min_n)) %>%\n  ggplot(aes(mtry, mean, color = min_n)) +\n  geom_line(alpha = 0.5, size = 1.5) +\n  geom_point() +\n  labs(y = \"Accuracy\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nWell that's interesting, lets see what tune thinks is best\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_82b38c2b896986f6da46bf7ca857f56f'}\n\n```{.r .cell-code}\nrf_best_params <- select_best(rf_grid_tune,\"accuracy\")\nrf_best_params %>% knitr::kable()\n```\n\n::: {.cell-output-display}\n| mtry| min_n|.config               |\n|----:|-----:|:---------------------|\n|   31|    17|Preprocessor1_Model14 |\n:::\n:::\n\n\n### RF Final Model\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_ab715ec8543edeb0aa6edc090e5c5606'}\n\n```{.r .cell-code}\nrf_final_model <- finalize_model(\n  rf_model,\n  rf_best_params\n)\nrf_final_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 31\n  trees = 1000\n  min_n = 17\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n```\n:::\n:::\n\n\n### RF Final Workflow\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-5_455d1d8c7a11cb64d9a7c620e1edc77a'}\n\n```{.r .cell-code}\nrf_final_wflow <- finalize_workflow(\n  rf_wflow,\n  rf_best_params\n)\n\nrf_final_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_dummy()\n• step_factor2string()\n• step_zv()\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 31\n  trees = 1000\n  min_n = 17\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n```\n:::\n:::\n\n\n### RF Parameter Importance\n\n\n::: {.cell hash='index_cache/html/rf_vip_d26a98b62579ccd45bed899f81a31fb7'}\n\n```{.r .cell-code}\nrf_final_wflow %>%\n  fit(data = train_proc_adj_tbl) %>%\n  extract_fit_parsnip() %>%\n  vip(geom = \"point\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rf_vip-1.png){width=672}\n:::\n:::\n\n\n### RF Final Fit\n\n\n::: {.cell hash='index_cache/html/rf_fit_bac7018783292a20c97c992519563565'}\n\n```{.r .cell-code}\nrf_final_fit <- \n  rf_final_wflow %>% \n  last_fit(train_split)\n\nrf_final_metrics <- collect_metrics(rf_final_fit)\nrf_final_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.821 Preprocessor1_Model1\n2 roc_auc  binary         0.874 Preprocessor1_Model1\n```\n:::\n:::\n\n\n### RF Predict\n\n\n::: {.cell hash='index_cache/html/rf_predict_cd8a43be420640ed169d2184abd8f3c4'}\n\n```{.r .cell-code}\n# rf_final_fit <- rf_wflow %>% fit(train_test)\n# class(rf_final_fit)\n\n rf_test_predictions <- \n   collect_predictions(rf_final_fit)\n   # fit(rf_final_wflow,train_train) %>% \n   # predict(rf_final_wflow, new_data = train_test) %>% \n   #bind_cols(predict(rf_final_wflow, train_test,type = \"prob\")) %>% \n   #bind_cols(train_test %>% select(survived))\n\n \n head(rf_test_predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 7\n  id               .pred_0 .pred_1  .row .pred_class survived .config           \n  <chr>              <dbl>   <dbl> <int> <fct>       <fct>    <chr>             \n1 train/test split  0.0101   0.990    10 1           1        Preprocessor1_Mod…\n2 train/test split  0.386    0.614    11 1           1        Preprocessor1_Mod…\n3 train/test split  0.703    0.297    14 0           0        Preprocessor1_Mod…\n4 train/test split  0.884    0.116    18 0           1        Preprocessor1_Mod…\n5 train/test split  0.370    0.630    20 1           1        Preprocessor1_Mod…\n6 train/test split  0.598    0.402    23 0           1        Preprocessor1_Mod…\n```\n:::\n:::\n\n\n### RF Performance on Test Set\n\n\n::: {.cell hash='index_cache/html/rf_perf_bf8b9af6d81bf6b1f88339fcf10c356c'}\n\n```{.r .cell-code}\n# rf_test_predictions %>% \n#   roc_auc(truth = survived, .pred_1,event_level = \"second\")\n\nrf_metrics_accuracy <- rf_test_predictions %>% \n  metrics(truth = survived, estimate = .pred_class) %>% \n  filter(.metric == \"accuracy\")\nrf_metrics_accuracy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.821\n```\n:::\n\n```{.r .cell-code}\nrf_test_predictions %>% \n  roc_curve(truth = survived, .pred_1,event_level = \"second\") %>% \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rf_perf-1.png){width=672}\n:::\n:::\n\n\n### RF Confusion Matrix\n\n\n::: {.cell hash='index_cache/html/rf_confusion_matrix_eaf1b481e40f0108bb070ba380e65033'}\n\n```{.r .cell-code}\nrf_test_predictions %>% conf_mat(survived,.pred_class) %>% \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rf_confusion_matrix-1.png){width=672}\n:::\n:::\n\n\n## XG Boost - Usemodel\n\n### XGB - Usemodel Library specs\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_96a31d4a862bd1f010351f7cde05da45'}\n\n```{.r .cell-code}\nlibrary(usemodels)\n\nuse_xgboost(survived ~ .,\n            data=train_train,\n            verbose = TRUE\n  \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nxgboost_recipe <- \n  recipe(formula = survived ~ ., data = train_train) %>% \n  step_novel(all_nominal_predictors()) %>% \n  ## This model requires the predictors to be numeric. The most common \n  ## method to convert qualitative predictors to numeric is to create \n  ## binary indicator variables (aka dummy variables) from these \n  ## predictors. However, for this model, binary indicator variables can be \n  ## made for each of the levels of the factors (known as 'one-hot \n  ## encoding'). \n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% \n  step_zv(all_predictors()) \n\nxgboost_spec <- \n  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), \n    loss_reduction = tune(), sample_size = tune()) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\") \n\nxgboost_workflow <- \n  workflow() %>% \n  add_recipe(xgboost_recipe) %>% \n  add_model(xgboost_spec) \n\nset.seed(19336)\nxgboost_tune <-\n  tune_grid(xgboost_workflow, resamples = stop(\"add your rsample object\"), grid = stop(\"add number of candidate points\"))\n```\n:::\n:::\n\n\n### XGB - Parameters\n\nThis grid is used for both versions of XG Boost.\n\n\n::: {.cell hash='index_cache/html/xgb_grid_680099f8ff2daff3423ffdce00b60695'}\n\n```{.r .cell-code}\nxgb_grid <- grid_latin_hypercube(\n  tree_depth(),\n  min_n(),\n  trees(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  finalize(mtry(), train_train),\n  learn_rate(),\n  size = 30\n)\n\nhead(xgb_grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 7\n  tree_depth min_n trees loss_reduction sample_size  mtry    learn_rate\n       <int> <int> <int>          <dbl>       <dbl> <int>         <dbl>\n1         14    30  1603   0.0230             0.806    13 0.00985      \n2         11     9    22   0.0000361          0.983     3 0.0000469    \n3          1    17   848   0.00581            0.539    11 0.00559      \n4         10     8  1097   0.00000104         0.652     8 0.00000000128\n5         11    19  1422   1.00               0.283     6 0.00124      \n6         15    32  1007   0.0000000318       0.919    15 0.00000608   \n```\n:::\n:::\n\n::: {.cell hash='index_cache/html/usemodel_scripts_02c4f55753ef3484e76bf58dffd3f69c'}\n\n```{.r .cell-code}\nxgboost_usemodel_recipe <- \n  recipe(formula = survived ~ ., data = train_train) %>% \n  step_novel(all_nominal_predictors()) %>% \n  ## This model requires the predictors to be numeric. The most common \n  ## method to convert qualitative predictors to numeric is to create \n  ## binary indicator variables (aka dummy variables) from these \n  ## predictors. However, for this model, binary indicator variables can be \n  ## made for each of the levels of the factors (known as 'one-hot \n  ## encoding'). \n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% \n  step_zv(all_predictors()) \n\nxgboost_usemodel_model <- \n  boost_tree(trees = tune(), mtry = tune(),min_n = tune(), tree_depth = tune(), learn_rate = tune(), \n    loss_reduction = tune(), sample_size = tune()) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\") \n\nxgboost_usemodel_wflow <- \n  workflow() %>% \n  add_recipe(xgboost_usemodel_recipe) %>% \n  add_model(xgboost_usemodel_model) \n\n#doParallel::registerDoParallel(cores = cores)\ncl <- parallel::makePSOCKcluster(cores - 1)\n\nset.seed(1234)\nxgboost_usemodel_tune <-\n  tune_grid(xgboost_usemodel_wflow, resamples = folds, grid = xgb_grid)\n\nparallel::stopCluster(cl)\n```\n:::\n\n\n### XGB - Usemodel Best Parameter Settings\n\n\n::: {.cell hash='index_cache/html/xgb_usemodel_para_sel_336c4fd119a9feb627337204df9065c4'}\n\n```{.r .cell-code}\nxgb_tuning_metrics_usemodel <- collect_metrics(xgboost_usemodel_tune)\nxgb_tuning_metrics_usemodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 × 13\n    mtry trees min_n tree_depth learn_r…¹ loss_r…² sampl…³ .metric .esti…⁴  mean\n   <int> <int> <int>      <int>     <dbl>    <dbl>   <dbl> <chr>   <chr>   <dbl>\n 1    11   848    17          1   5.59e-3 5.81e- 3   0.539 accura… binary  0.708\n 2    11   848    17          1   5.59e-3 5.81e- 3   0.539 roc_auc binary  0.823\n 3     9  1145    10          2   5.20e-8 1.60e- 1   0.866 accura… binary  0.702\n 4     9  1145    10          2   5.20e-8 1.60e- 1   0.866 roc_auc binary  0.821\n 5    17   740    15          2   8.39e-2 6.64e- 9   0.252 accura… binary  0.743\n 6    17   740    15          2   8.39e-2 6.64e- 9   0.252 roc_auc binary  0.776\n 7    12   690    38          2   3.89e-9 5.16e-10   0.146 accura… binary  0.616\n 8    12   690    38          2   3.89e-9 5.16e-10   0.146 roc_auc binary  0.5  \n 9    10  1314    11          3   1.72e-5 1.25e- 1   0.163 accura… binary  0.616\n10    10  1314    11          3   1.72e-5 1.25e- 1   0.163 roc_auc binary  0.757\n# … with 50 more rows, 3 more variables: n <int>, std_err <dbl>, .config <chr>,\n#   and abbreviated variable names ¹​learn_rate, ²​loss_reduction, ³​sample_size,\n#   ⁴​.estimator\n```\n:::\n\n```{.r .cell-code}\nxgboost_usemodel_tune %>%\n  collect_metrics() %>%\n  filter(.metric == \"accuracy\") %>%\n  select(mean, mtry:sample_size) %>%\n  pivot_longer(mtry:sample_size,\n               values_to = \"value\",\n               names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"Accuracy\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/xgb_usemodel_para_sel-1.png){width=672}\n:::\n:::\n\n\nNow select best from above\n\n\n::: {.cell hash='index_cache/html/xgb_usemodel_select_paras_f1d2ad1846be1fb5c1f161eddc7d881b'}\n\n```{.r .cell-code}\nshow_best(xgboost_usemodel_tune, \"accuracy\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 13\n   mtry trees min_n tree_d…¹ learn…² loss_…³ sampl…⁴ .metric .esti…⁵  mean     n\n  <int> <int> <int>    <int>   <dbl>   <dbl>   <dbl> <chr>   <chr>   <dbl> <int>\n1    13  1603    30       14 9.85e-3 2.30e-2   0.806 accura… binary  0.766     5\n2     4  1982     4       15 4.01e-2 5.53e-8   0.393 accura… binary  0.754     5\n3    17   740    15        2 8.39e-2 6.64e-9   0.252 accura… binary  0.743     5\n4    11   848    17        1 5.59e-3 5.81e-3   0.539 accura… binary  0.708     5\n5     9  1145    10        2 5.20e-8 1.60e-1   0.866 accura… binary  0.702     5\n# … with 2 more variables: std_err <dbl>, .config <chr>, and abbreviated\n#   variable names ¹​tree_depth, ²​learn_rate, ³​loss_reduction, ⁴​sample_size,\n#   ⁵​.estimator\n```\n:::\n\n```{.r .cell-code}\nxgb_usemodel_best_params <- select_best(xgboost_usemodel_tune, \"accuracy\")\nxgb_usemodel_best_params\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 8\n   mtry trees min_n tree_depth learn_rate loss_reduction sample_size .config    \n  <int> <int> <int>      <int>      <dbl>          <dbl>       <dbl> <chr>      \n1    13  1603    30         14    0.00985         0.0230       0.806 Preprocess…\n```\n:::\n\n```{.r .cell-code}\nxgb_usemodel_final_wflow <- finalize_workflow(\n  xgboost_usemodel_wflow,\n  xgb_usemodel_best_params\n)\n\nxgb_usemodel_final_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = 13\n  trees = 1603\n  min_n = 30\n  tree_depth = 14\n  learn_rate = 0.00985014124434902\n  loss_reduction = 0.0230337047700143\n  sample_size = 0.80635308077326\n\nComputational engine: xgboost \n```\n:::\n:::\n\n\n### XGB - Usemodel Parameter Ranking - VIP\n\n\n::: {.cell hash='index_cache/html/xgb_usemodel_vip_848bb5313938772ae48ef8719cafa51e'}\n\n```{.r .cell-code}\nxgb_usemodel_final_wflow %>%\n  fit(data = train_train) %>%\n  extract_fit_parsnip() %>%\n  vip(geom = \"point\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/xgb_usemodel_vip-1.png){width=672}\n:::\n:::\n\n\n### XGB - Usemodel Performance\n\n#### XGB - Usemodel Accuracy Measured on Test Set\n\n\n::: {.cell hash='index_cache/html/xgb_usemodel_final_metrics_3e5c56ec629778ed6af3ff07ea2b0e87'}\n\n```{.r .cell-code}\ncl <- parallel::makePSOCKcluster(cores - 1)\n\nset.seed(1234)\nxgb_usemodel_final_res <- last_fit(xgb_usemodel_final_wflow, train_split)\nxgb_usemodel_final_res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits            id               .metrics .notes   .predictions .workflow \n  <list>            <chr>            <list>   <list>   <list>       <list>    \n1 <split [667/224]> train/test split <tibble> <tibble> <tibble>     <workflow>\n\nThere were issues with some computations:\n\n  - Warning(s) x2: There are new levels in a factor: NA\n\nRun `show_notes(.Last.tune.result)` for more information.\n```\n:::\n\n```{.r .cell-code}\nxgb_usemodel_final_metrics <- collect_metrics(xgb_usemodel_final_res)\nxgb_usemodel_final_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.670 Preprocessor1_Model1\n2 roc_auc  binary         0.797 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\nparallel::stopCluster(cl)\n```\n:::\n\n\n#### XGB - Usemodel AUC on Test Set (within train)\n\n\n::: {.cell hash='index_cache/html/xgb_usemodel_auc_6712220969b082a61f039f222629bfdf'}\n\n```{.r .cell-code}\nxgb_usemodel_final_res %>%\n  collect_predictions() %>%\n  roc_curve( truth = survived,.pred_1, event_level = \"second\") %>%\n  ggplot(aes(x = 1-specificity, y = sensitivity)) +\n  geom_line(size = 1.5, color = \"midnightblue\") +\n  geom_abline(\n    lty = 2, alpha = 0.5,\n    color = \"gray50\",\n    size = 1.2\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/xgb_usemodel_auc-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-7_e04c090ef2f29d52d54387e63ff5d60f'}\n\n```{.r .cell-code}\nxgb_usemodel_test_predictions <- collect_predictions(xgb_usemodel_final_res)\nhead(xgb_usemodel_test_predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 7\n  id               .pred_0 .pred_1  .row .pred_class survived .config           \n  <chr>              <dbl>   <dbl> <int> <fct>       <fct>    <chr>             \n1 train/test split   0.534   0.466    10 0           1        Preprocessor1_Mod…\n2 train/test split   0.291   0.709    11 1           1        Preprocessor1_Mod…\n3 train/test split   0.733   0.267    14 0           0        Preprocessor1_Mod…\n4 train/test split   0.736   0.264    18 0           1        Preprocessor1_Mod…\n5 train/test split   0.640   0.360    20 0           1        Preprocessor1_Mod…\n6 train/test split   0.625   0.375    23 0           1        Preprocessor1_Mod…\n```\n:::\n:::\n\n\n### XGB - Usemodel Confusion Matrix\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-8_facea20d8062b6e78a5ce0654906498e'}\n\n```{.r .cell-code}\nxgb_usemodel_test_predictions %>% conf_mat(survived,.pred_class) %>% \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## XG Boost - Base Recipe\n\n### XGB Model Spec\n\n\n::: {.cell hash='index_cache/html/xgb_model_9ec3e3e8713ed1259c7033da09899688'}\n\n```{.r .cell-code}\nxgb_model <- \n  boost_tree(\n    trees = tune(),\n    tree_depth = tune(),\n    min_n = tune(),\n    loss_reduction = tune(),\n    sample_size = tune(),\n    mtry = tune(),\n    learn_rate = tune()) %>% \n  set_engine(\"xgboost\") %>% \n  set_mode(\"classification\")\n\nxgb_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = tune()\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n```\n:::\n:::\n\n\n### XGB Workflow\n\n\n::: {.cell hash='index_cache/html/xgb_wflow_3ff529f87986d8f416e395e607d02eef'}\n\n```{.r .cell-code}\nxgb_wflow <- \n  workflow() %>% \n  add_model(xgb_model) %>% \n  add_recipe(recipe_base)\n```\n:::\n\n\n### XGB Hyper-Parameter Tuning\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-9_ce66d6594ccb6ec16a66f23283e0cebf'}\n\n```{.r .cell-code}\n# xgb_folds <- vfold_cv(training(train_split), strata = survived)\n# xgb_folds\n\n\n#doParallel::registerDoParallel(cores = cores)\n\nset.seed(1234)\ncl <- parallel::makePSOCKcluster(cores - 1)\n\nxgb_tuning_result <- tune_grid(\n  xgb_wflow,\n  resamples = folds,\n  grid      = xgb_grid,\n  control  = control_grid(save_pred = TRUE,save_workflow = TRUE)\n)\nxgb_tuning_result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 5\n  splits            id    .metrics           .notes           .predictions\n  <list>            <chr> <list>             <list>           <list>      \n1 <split [532/135]> Fold1 <tibble [60 × 11]> <tibble [0 × 3]> <tibble>    \n2 <split [534/133]> Fold2 <tibble [60 × 11]> <tibble [0 × 3]> <tibble>    \n3 <split [534/133]> Fold3 <tibble [60 × 11]> <tibble [0 × 3]> <tibble>    \n4 <split [534/133]> Fold4 <tibble [60 × 11]> <tibble [0 × 3]> <tibble>    \n5 <split [534/133]> Fold5 <tibble [60 × 11]> <tibble [0 × 3]> <tibble>    \n```\n:::\n\n```{.r .cell-code}\nparallel::stopCluster(cl)\n```\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-10_f44b7b3d25366d3f8d364cbd7733b3b9'}\n\n```{.r .cell-code}\nxgb_tuning_metrics <- collect_metrics(xgb_tuning_result)\nxgb_tuning_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 × 13\n    mtry trees min_n tree_depth learn_r…¹ loss_r…² sampl…³ .metric .esti…⁴  mean\n   <int> <int> <int>      <int>     <dbl>    <dbl>   <dbl> <chr>   <chr>   <dbl>\n 1    11   848    17          1   5.59e-3 5.81e- 3   0.539 accura… binary  0.775\n 2    11   848    17          1   5.59e-3 5.81e- 3   0.539 roc_auc binary  0.844\n 3     9  1145    10          2   5.20e-8 1.60e- 1   0.866 accura… binary  0.775\n 4     9  1145    10          2   5.20e-8 1.60e- 1   0.866 roc_auc binary  0.842\n 5    17   740    15          2   8.39e-2 6.64e- 9   0.252 accura… binary  0.718\n 6    17   740    15          2   8.39e-2 6.64e- 9   0.252 roc_auc binary  0.753\n 7    12   690    38          2   3.89e-9 5.16e-10   0.146 accura… binary  0.616\n 8    12   690    38          2   3.89e-9 5.16e-10   0.146 roc_auc binary  0.5  \n 9    10  1314    11          3   1.72e-5 1.25e- 1   0.163 accura… binary  0.616\n10    10  1314    11          3   1.72e-5 1.25e- 1   0.163 roc_auc binary  0.721\n# … with 50 more rows, 3 more variables: n <int>, std_err <dbl>, .config <chr>,\n#   and abbreviated variable names ¹​learn_rate, ²​loss_reduction, ³​sample_size,\n#   ⁴​.estimator\n```\n:::\n\n```{.r .cell-code}\nxgb_tuning_result %>%\n  collect_metrics() %>%\n  filter(.metric == \"accuracy\") %>%\n  select(mean, mtry:sample_size) %>%\n  pivot_longer(mtry:sample_size,\n               values_to = \"value\",\n               names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n#### XGB Best Parameters then Finalise Workflow\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-11_93d3423cf986b698b500ed4cc71f3ccd'}\n\n```{.r .cell-code}\nshow_best(xgb_tuning_result, \"accuracy\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 13\n   mtry trees min_n tree_d…¹ learn…² loss_…³ sampl…⁴ .metric .esti…⁵  mean     n\n  <int> <int> <int>    <int>   <dbl>   <dbl>   <dbl> <chr>   <chr>   <dbl> <int>\n1     4  1982     4       15 4.01e-2 5.53e-8   0.393 accura… binary  0.814     5\n2    15  1689     7       13 2.02e-9 5.67e-6   0.958 accura… binary  0.784     5\n3    16  1549     6        7 1.36e-4 4.92e-4   0.463 accura… binary  0.783     5\n4    19   259    16        9 6.05e-7 1.64e-4   0.735 accura… binary  0.781     5\n5    13  1603    30       14 9.85e-3 2.30e-2   0.806 accura… binary  0.780     5\n# … with 2 more variables: std_err <dbl>, .config <chr>, and abbreviated\n#   variable names ¹​tree_depth, ²​learn_rate, ³​loss_reduction, ⁴​sample_size,\n#   ⁵​.estimator\n```\n:::\n\n```{.r .cell-code}\nxgb_best_params <- select_best(xgb_tuning_result, \"accuracy\")\nxgb_best_params\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 8\n   mtry trees min_n tree_depth learn_rate loss_reduction sample_size .config    \n  <int> <int> <int>      <int>      <dbl>          <dbl>       <dbl> <chr>      \n1     4  1982     4         15     0.0401   0.0000000553       0.393 Preprocess…\n```\n:::\n\n```{.r .cell-code}\nxgb_final_wflow <- finalize_workflow(\n  xgb_wflow,\n  xgb_best_params\n)\n\nxgb_final_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_dummy()\n• step_factor2string()\n• step_zv()\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = 4\n  trees = 1982\n  min_n = 4\n  tree_depth = 15\n  learn_rate = 0.0400670375292599\n  loss_reduction = 5.52655767061452e-08\n  sample_size = 0.392634701682255\n\nComputational engine: xgboost \n```\n:::\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-12_b19308e3b0234c5328ff9a7135a57d29'}\n\n```{.r .cell-code}\nxgb_final_wflow %>%\n  fit(data = train_train) %>%\n  extract_fit_parsnip() %>%\n  vip(geom = \"point\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n### XGB Performance on Training Test Set\n\n#### XGB Accuracy Measured on Test Set\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-13_06d333f0b8c5c926b83e25badaa6314d'}\n\n```{.r .cell-code}\nxgb_final_res <- last_fit(xgb_final_wflow, train_split)\nxgb_final_res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits            id               .metrics .notes   .predictions .workflow \n  <list>            <chr>            <list>   <list>   <list>       <list>    \n1 <split [667/224]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n\n```{.r .cell-code}\nxgb_final_metrics <- collect_metrics(xgb_final_res)\nxgb_final_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.839 Preprocessor1_Model1\n2 roc_auc  binary         0.870 Preprocessor1_Model1\n```\n:::\n:::\n\n\n#### XGB AUC on Test Set (within train)\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-14_dfbced372120513573db0fd6be05ab27'}\n\n```{.r .cell-code}\nxgb_final_res %>%\n  collect_predictions() %>%\n  roc_curve( truth = survived,.pred_1, event_level = \"second\") %>%\n  ggplot(aes(x = 1-specificity, y = sensitivity)) +\n  geom_line(size = 1.5, color = \"midnightblue\") +\n  geom_abline(\n    lty = 2, alpha = 0.5,\n    color = \"gray50\",\n    size = 1.2\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-15_a601d6d9ec57c2059220b0efad8a753b'}\n\n```{.r .cell-code}\nxgb_test_predictions <- collect_predictions(xgb_final_res)\nhead(xgb_test_predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 7\n  id               .pred_0 .pred_1  .row .pred_class survived .config           \n  <chr>              <dbl>   <dbl> <int> <fct>       <fct>    <chr>             \n1 train/test split  0.0609  0.939     10 1           1        Preprocessor1_Mod…\n2 train/test split  0.346   0.654     11 1           1        Preprocessor1_Mod…\n3 train/test split  0.968   0.0321    14 0           0        Preprocessor1_Mod…\n4 train/test split  0.845   0.155     18 0           1        Preprocessor1_Mod…\n5 train/test split  0.385   0.615     20 1           1        Preprocessor1_Mod…\n6 train/test split  0.319   0.681     23 1           1        Preprocessor1_Mod…\n```\n:::\n:::\n\n\n### XGB Confusion Matrix\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-16_652ab025b4febc26c286168a4eed3637'}\n\n```{.r .cell-code}\nxgb_test_predictions %>% conf_mat(survived,.pred_class) %>% \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Neural Net\n\n### NN Model\n\n\n::: {.cell hash='index_cache/html/nn_model_9e8adcbd4e56ec629568772b4fb06e6e'}\n\n```{.r .cell-code}\nnnet_model <- \n   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% \n   set_engine(\"nnet\", MaxNWts = 2600) %>% \n   set_mode(\"classification\")\n\nnnet_model %>% translate()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle Layer Neural Network Model Specification (classification)\n\nMain Arguments:\n  hidden_units = tune()\n  penalty = tune()\n  epochs = tune()\n\nEngine-Specific Arguments:\n  MaxNWts = 2600\n\nComputational engine: nnet \n\nModel fit template:\nnnet::nnet(formula = missing_arg(), data = missing_arg(), size = tune(), \n    decay = tune(), maxit = tune(), MaxNWts = 2600, trace = FALSE, \n    linout = FALSE)\n```\n:::\n:::\n\n\n### NN Workflow\n\n\n::: {.cell hash='index_cache/html/nn_wflow_ec7d673f471634628481fb743e44d23a'}\n\n```{.r .cell-code}\nnnet_wflow <- workflow() %>% \n  add_model(nnet_model) %>% \n  add_recipe(recipe_base)\n```\n:::\n\n\n### NN Parameters\n\n\n::: {.cell hash='index_cache/html/nn_params_45d5dfbacca71b0e26302990669e2a24'}\n\n```{.r .cell-code}\nnnet_grid <- grid_latin_hypercube(\n  hidden_units(),\n  penalty (),\n  epochs ()\n)\n\nhead(nnet_grid) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  hidden_units  penalty epochs\n         <int>    <dbl>  <int>\n1            7 7.66e- 5    944\n2            6 6.36e-10    524\n3            2 1.80e- 3    146\n```\n:::\n:::\n\n\n### NN Hyper-Parameter Tuning\n\n\n::: {.cell hash='index_cache/html/nn_tuning_c838257f6c9515ac126b6e25a479a45b'}\n\n```{.r .cell-code}\n# nnet_folds <- vfold_cv(train_train, strata = survived)\n# nnet_folds\n\n\n# doParallel::registerDoParallel(cores = cores)\ncl <- parallel::makePSOCKcluster(cores - 1)\n\nset.seed(1234)\nnnet_tuning_result <- tune_grid(\n  nnet_wflow,\n  resamples = folds,\n  grid      = nnet_grid,\n  control   = control_grid(save_pred = TRUE,save_workflow = TRUE)\n)\nnnet_tuning_result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 5\n  splits            id    .metrics         .notes           .predictions      \n  <list>            <chr> <list>           <list>           <list>            \n1 <split [532/135]> Fold1 <tibble [6 × 7]> <tibble [0 × 3]> <tibble [405 × 9]>\n2 <split [534/133]> Fold2 <tibble [6 × 7]> <tibble [0 × 3]> <tibble [399 × 9]>\n3 <split [534/133]> Fold3 <tibble [6 × 7]> <tibble [0 × 3]> <tibble [399 × 9]>\n4 <split [534/133]> Fold4 <tibble [6 × 7]> <tibble [0 × 3]> <tibble [399 × 9]>\n5 <split [534/133]> Fold5 <tibble [6 × 7]> <tibble [0 × 3]> <tibble [399 × 9]>\n```\n:::\n\n```{.r .cell-code}\nparallel::stopCluster(cl)\n```\n:::\n\n\n### NN Best Parameters and Finalise Workflow\n\n\n::: {.cell hash='index_cache/html/nn_best_params_0713e28c85a34f96b58612b422201c47'}\n\n```{.r .cell-code}\nshow_best(nnet_tuning_result, \"accuracy\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 9\n  hidden_units  penalty epochs .metric  .estimator  mean     n std_err .config  \n         <int>    <dbl>  <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>    \n1            2 1.80e- 3    146 accuracy binary     0.820     5  0.0161 Preproce…\n2            7 7.66e- 5    944 accuracy binary     0.776     5  0.0372 Preproce…\n3            6 6.36e-10    524 accuracy binary     0.766     5  0.0334 Preproce…\n```\n:::\n\n```{.r .cell-code}\nnn_best_params <- select_best(nnet_tuning_result, \"accuracy\")\n\nnnet_best_auc <- select_best(xgb_tuning_result, \"accuracy\")\nnnet_best_auc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 8\n   mtry trees min_n tree_depth learn_rate loss_reduction sample_size .config    \n  <int> <int> <int>      <int>      <dbl>          <dbl>       <dbl> <chr>      \n1     4  1982     4         15     0.0401   0.0000000553       0.393 Preprocess…\n```\n:::\n\n```{.r .cell-code}\nnnet_final_wflow <- finalize_workflow(\n  nnet_wflow,\n  nn_best_params\n)\n\nnnet_final_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: mlp()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_impute_knn()\n• step_dummy()\n• step_factor2string()\n• step_zv()\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nSingle Layer Neural Network Model Specification (classification)\n\nMain Arguments:\n  hidden_units = 2\n  penalty = 0.00180188446786651\n  epochs = 146\n\nEngine-Specific Arguments:\n  MaxNWts = 2600\n\nComputational engine: nnet \n```\n:::\n:::\n\n::: {.cell hash='index_cache/html/nn_final_train_0e02e51a588f3f7bfc50148e3d1d5717'}\n\n```{.r .cell-code}\nnnet_final_wflow %>%\n  fit(data = train_train) %>%\n  extract_fit_parsnip() %>%\n  vip(geom = \"point\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/nn_final_train-1.png){width=672}\n:::\n:::\n\n\n### NN Accuracy - Train/Test Set\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-17_9beee9d59f301812b83a4f0c982183ad'}\n\n```{.r .cell-code}\nnnet_tuning_metrics <- collect_metrics(nnet_tuning_result)\nnnet_tuning_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 9\n  hidden_units  penalty epochs .metric  .estimator  mean     n std_err .config  \n         <int>    <dbl>  <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>    \n1            7 7.66e- 5    944 accuracy binary     0.776     5  0.0372 Preproce…\n2            7 7.66e- 5    944 roc_auc  binary     0.788     5  0.0395 Preproce…\n3            6 6.36e-10    524 accuracy binary     0.766     5  0.0334 Preproce…\n4            6 6.36e-10    524 roc_auc  binary     0.779     5  0.0421 Preproce…\n5            2 1.80e- 3    146 accuracy binary     0.820     5  0.0161 Preproce…\n6            2 1.80e- 3    146 roc_auc  binary     0.865     5  0.0153 Preproce…\n```\n:::\n\n```{.r .cell-code}\nnnet_final_res <- last_fit(nnet_final_wflow, train_split)\nnnet_final_res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits            id               .metrics .notes   .predictions .workflow \n  <list>            <chr>            <list>   <list>   <list>       <list>    \n1 <split [667/224]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n\n```{.r .cell-code}\nnnet_final_metrics <- collect_metrics(nnet_final_res)\nnnet_final_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.786 Preprocessor1_Model1\n2 roc_auc  binary         0.824 Preprocessor1_Model1\n```\n:::\n:::\n\n\n### NN AUC\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-18_5bf02ddb00ffbddc4b00b129f9e7e3d1'}\n\n```{.r .cell-code}\nnnet_final_res %>%\n  collect_predictions() %>%\n  roc_curve( truth = survived,.pred_1, event_level = \"second\") %>%\n  ggplot(aes(x = 1-specificity, y = sensitivity)) +\n  geom_line(size = 1.5, color = \"midnightblue\") +\n  geom_abline(\n    lty = 2, alpha = 0.5,\n    color = \"gray50\",\n    size = 1.2\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n### NN Predictions on Train/Test Set\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-19_9b6d3df779586ce4bc7b3b67a54ab2a7'}\n\n```{.r .cell-code}\nnnet_test_predictions <- nnet_final_res %>%\n  collect_predictions() \nhead(nnet_test_predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 7\n  id               .pred_0 .pred_1  .row .pred_class survived .config           \n  <chr>              <dbl>   <dbl> <int> <fct>       <fct>    <chr>             \n1 train/test split   0.363   0.637    10 1           1        Preprocessor1_Mod…\n2 train/test split   0.603   0.397    11 0           1        Preprocessor1_Mod…\n3 train/test split   0.701   0.299    14 0           0        Preprocessor1_Mod…\n4 train/test split   0.680   0.320    18 0           1        Preprocessor1_Mod…\n5 train/test split   0.363   0.637    20 1           1        Preprocessor1_Mod…\n6 train/test split   0.363   0.637    23 1           1        Preprocessor1_Mod…\n```\n:::\n:::\n\n\n### NN Confusion Matrix\n\n\n::: {.cell hash='index_cache/html/NN_confusion_matrix_52e62d9b1c085ac725482c7f89bb018c'}\n\n```{.r .cell-code}\nnnet_test_predictions %>% conf_mat(survived,.pred_class) %>% \n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/NN_confusion_matrix-1.png){width=672}\n:::\n:::\n\n\n## Stack Models\n\n### Stack Recipe\n\n\n::: {.cell hash='index_cache/html/stack_recipe_0163952d53ce58e4fc42acb6a4beace2'}\n\n```{.r .cell-code}\nrecipe_stack <- \n  recipe(survived ~ ., data = train_train) %>% \n  update_role(passenger_id, name,surname,ticket,cabin,new_role = \"ID\") %>% \n  step_impute_knn(all_numeric_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>%\n  step_factor2string(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_pca()\nrecipe_stack\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        ID          5\n   outcome          1\n predictor         13\n\nOperations:\n\nK-nearest neighbor imputation for all_numeric_predictors()\nDummy variables from all_nominal_predictors()\nCharacter variables from all_nominal_predictors()\nZero variance filter on all_predictors()\nPCA extraction with <none>\n```\n:::\n\n```{.r .cell-code}\nrecipe_stack_trained <- prep(recipe_base)\nrecipe_stack_trained\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        ID          5\n   outcome          1\n predictor         13\n\nTraining data contained 667 data points and 514 incomplete rows. \n\nOperations:\n\nK-nearest neighbor imputation for age, sib_sp, parch, fare [trained]\nDummy variables from pclass, sex, embarked, train_test, pax_type, cabin_prefac... [trained]\nCharacter variables from <none> [trained]\nZero variance filter removed train_test_test [trained]\nNo PCA components were extracted from <none> [trained]\n```\n:::\n:::\n\n\n### Stack Controls\n\n\n::: {.cell hash='index_cache/html/stack_controls_168dee138f0c3d45b9b28d6b88467c5a'}\n\n```{.r .cell-code}\nstack_ctrl <- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n#stack_folds <- vfold_cv(training(train_split), v=10,strata = \"survived\")\n\nlibrary(stacks)\n\nmodel_stack <-\n  stacks() %>%\n  #add_candidates(lr_wflow) %>%\n  #add_candidates(rf_wflow) %>%\n  add_candidates(nnet_tuning_result) %>%\n  add_candidates(rlr_tuning_result) %>% \n  add_candidates(xgb_tuning_result)\n```\n:::\n\n\n### Stack Blend\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-20_779eba3ed93d1f17cc10366dca590cd5'}\n\n```{.r .cell-code}\ncl <- parallel::makePSOCKcluster(cores - 1)\n\nset.seed(1234)\nensemble <- blend_predictions(model_stack,penalty = 10^seq(-2, -0.5, length = 20))\nautoplot(ensemble)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n```{.r .cell-code}\nparallel::stopCluster(cl)\n```\n:::\n\n::: {.cell hash='index_cache/html/ensemble_table_568ad5b61c8fb9bdb475ea2105dbc201'}\n\n```{.r .cell-code}\nensemble \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n  member                         type         weight\n  <chr>                          <chr>         <dbl>\n1 .pred_1_nnet_tuning_result_1_3 mlp           2.25 \n2 .pred_1_rlr_tuning_result_1_30 logistic_reg  1.11 \n3 .pred_1_rlr_tuning_result_1_28 logistic_reg  1.09 \n4 .pred_1_xgb_tuning_result_1_29 boost_tree    0.658\n```\n:::\n:::\n\n\n### Stack Weights\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-21_641339cb787c021e1b28f8debbf3a1ec'}\n\n```{.r .cell-code}\nautoplot(ensemble, \"weights\") +\n  geom_text(aes(x = weight + 0.01, label = model), hjust = 0) + \n  theme(legend.position = \"none\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n### Fit Member Models\n\n\n::: {.cell hash='index_cache/html/fit_ensemble_ca764c3438cef8af871e28ef8833c1aa'}\n\n```{.r .cell-code}\nensemble <- fit_members(ensemble)\ncollect_parameters(ensemble,\"xgb_tuning_result\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 27 × 10\n   member          mtry trees min_n tree_…¹ learn…² loss_r…³ sampl…⁴ terms  coef\n   <chr>          <int> <int> <int>   <int>   <dbl>    <dbl>   <dbl> <chr> <dbl>\n 1 xgb_tuning_re…    11   848    17       1 5.59e-3 5.81e- 3   0.539 .pre…     0\n 2 xgb_tuning_re…     9  1145    10       2 5.20e-8 1.60e- 1   0.866 .pre…     0\n 3 xgb_tuning_re…    17   740    15       2 8.39e-2 6.64e- 9   0.252 .pre…     0\n 4 xgb_tuning_re…    10  1314    11       3 1.72e-5 1.25e- 1   0.163 .pre…     0\n 5 xgb_tuning_re…    18  1475    25       3 1.50e-6 2.46e- 9   0.328 .pre…     0\n 6 xgb_tuning_re…     6    98    23       4 4.07e-4 1.02e- 3   0.897 .pre…     0\n 7 xgb_tuning_re…     7   923    13       5 1.66e-3 1.78e- 5   0.352 .pre…     0\n 8 xgb_tuning_re…    16   610    26       5 1.14e-5 1.88e-10   0.211 .pre…     0\n 9 xgb_tuning_re…    16  1549     6       7 1.36e-4 4.92e- 4   0.463 .pre…     0\n10 xgb_tuning_re…    14  1900    22       7 2.14e-7 3.46e- 6   0.447 .pre…     0\n# … with 17 more rows, and abbreviated variable names ¹​tree_depth, ²​learn_rate,\n#   ³​loss_reduction, ⁴​sample_size\n```\n:::\n:::\n\n\n### Stack Predict\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-22_5310a4fa01757847b2bb0ac9ca6de754'}\n\n```{.r .cell-code}\n#ensemble_metrics <- metric_set(roc_auc,accuracy)\n\nensemble_test_predictions <- \n  predict(ensemble,train_test) %>% \n  bind_cols(train_test) \n\n\n# ensemble_test_predictions <- ensemble_test_predictions %>% \n#   mutate(.pred_class=as.numeric(.pred_class)) %>% \n#    mutate(survived =as.numeric(survived)) \n# \n# ensemble_test_predictions <- ensemble_test_predictions %>% \n#   mutate(roc = roc_auc(truth=survived, estimate = .pred_class))\n\n\n\nglimpse(ensemble_test_predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 224\nColumns: 20\n$ .pred_class   <fct> 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,…\n$ passenger_id  <dbl> 10, 11, 14, 18, 20, 23, 28, 40, 41, 51, 54, 57, 61, 66, …\n$ survived      <fct> 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,…\n$ pclass        <fct> 2, 3, 3, 2, 3, 3, 1, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3,…\n$ name          <fct> \"Nasser, Mrs. Nicholas (Adele Achem)\", \"Sandstrom, Miss.…\n$ sex           <fct> female, female, male, male, female, female, male, female…\n$ age           <dbl> 14, 4, 39, 30, 31, 15, 19, 14, 40, 7, 29, 21, 22, 6, 21,…\n$ sib_sp        <dbl> 1, 1, 1, 0, 0, 0, 3, 1, 1, 4, 1, 0, 0, 1, 0, 0, 0, 0, 3,…\n$ parch         <dbl> 0, 1, 5, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ ticket        <fct> 237736, PP 9549, 347082, 244373, 2649, 330923, 19950, 26…\n$ fare          <dbl> 30.0708, 16.7000, 31.2750, 13.0000, 7.2250, 8.0292, 263.…\n$ cabin         <fct> NA, G6, NA, NA, NA, NA, C23 C25 C27, NA, NA, NA, NA, NA,…\n$ embarked      <fct> C, S, S, S, C, Q, S, C, S, S, S, S, C, C, S, S, Q, S, S,…\n$ train_test    <fct> train, train, train, train, train, train, train, train, …\n$ pax_type      <fct> F_married, F_unmarried, Mr., Mr., F_married, F_unmarried…\n$ surname       <fct> \"Nasser,\", \"Sandstrom,\", \"Andersson,\", \"Williams,\", \"Mas…\n$ cabin_preface <fct> nk, G, nk, nk, nk, nk, C, nk, nk, nk, nk, nk, nk, nk, nk…\n$ ticket_group  <fct> couple, group, group, single, single, single, group, cou…\n$ family_group  <ord> couple, family, family, single, single, single, family, …\n$ age_group     <ord> teen, child, 30s, 30s, 30s, teen, teen, teen, 40s, child…\n```\n:::\n:::\n\n\n## Join Model Prediction Data\n\n\n::: {.cell hash='index_cache/html/all_predictions_802863b5f833397354bf01b521e3a9f8'}\n\n```{.r .cell-code}\nall_predictions <- \n  lr_test_predictions %>% mutate(model = \"LR\") %>% \n  bind_rows(nnet_test_predictions %>% mutate(model = \"NNet\")) %>% \n  bind_rows(rlr_test_predictions %>% mutate(model = \"Reg_LR\")) %>% \n  bind_rows(rf_test_predictions %>% mutate(model = \"RF\")) %>% \n  bind_rows(xgb_test_predictions %>% mutate(model = \"xgb\")) %>% \n  bind_rows(xgb_usemodel_test_predictions %>% mutate(model = \"xgb_usemodel\")) %>% \n  bind_rows(ensemble_test_predictions %>% mutate(model = \"ensemble\"))\n  \nall_predictions %>% head() %>% knitr::kable()\n```\n\n::: {.cell-output-display}\n|id               |   .pred_0|   .pred_1| .row|.pred_class |survived_pred |.config              | passenger_id|survived |pclass |name                                |sex    | age| sib_sp| parch|ticket  |    fare|cabin |embarked |train_test |pax_type    |surname     |cabin_preface |ticket_group |family_group |age_group |model |\n|:----------------|---------:|---------:|----:|:-----------|:-------------|:--------------------|------------:|:--------|:------|:-----------------------------------|:------|---:|------:|-----:|:-------|-------:|:-----|:--------|:----------|:-----------|:-----------|:-------------|:------------|:------------|:---------|:-----|\n|train/test split | 0.0947816| 0.9052184|   10|1           |1             |Preprocessor1_Model1 |           10|1        |2      |Nasser, Mrs. Nicholas (Adele Achem) |female |  14|      1|     0|237736  | 30.0708|NA    |C        |train      |F_married   |Nasser,     |nk            |couple       |couple       |teen      |LR    |\n|train/test split | 0.9999997| 0.0000003|   11|0           |1             |Preprocessor1_Model1 |           11|1        |3      |Sandstrom, Miss. Marguerite Rut     |female |   4|      1|     1|PP 9549 | 16.7000|G6    |S        |train      |F_unmarried |Sandstrom,  |G             |group        |family       |child     |LR    |\n|train/test split | 0.9959939| 0.0040061|   14|0           |0             |Preprocessor1_Model1 |           14|0        |3      |Andersson, Mr. Anders Johan         |male   |  39|      1|     5|347082  | 31.2750|NA    |S        |train      |Mr.         |Andersson,  |nk            |group        |family       |30s       |LR    |\n|train/test split | 0.7485089| 0.2514911|   18|0           |1             |Preprocessor1_Model1 |           18|1        |2      |Williams, Mr. Charles Eugene        |male   |  30|      0|     0|244373  | 13.0000|NA    |S        |train      |Mr.         |Williams,   |nk            |single       |single       |30s       |LR    |\n|train/test split | 0.1223361| 0.8776639|   20|1           |1             |Preprocessor1_Model1 |           20|1        |3      |Masselmani, Mrs. Fatima             |female |  31|      0|     0|2649    |  7.2250|NA    |C        |train      |F_married   |Masselmani, |nk            |single       |single       |30s       |LR    |\n|train/test split | 0.3091538| 0.6908462|   23|1           |1             |Preprocessor1_Model1 |           23|1        |3      |McGowan, Miss. Anna \"Annie\"         |female |  15|      0|     0|330923  |  8.0292|NA    |Q        |train      |F_unmarried |McGowan,    |nk            |single       |single       |teen      |LR    |\n:::\n:::\n\n\n## All Metrics\n\nOrdered by descending Accuracy metric\n\n\n::: {.cell hash='index_cache/html/all_metrics_bb59d6471849037d4a5ae2d9c55a8b12'}\n\n```{.r .cell-code}\nall_metrics <- \n  lr_final_metrics %>% mutate(model = \"LR\") %>% \n  bind_rows(nnet_final_metrics %>% mutate(model = \"NNet\")) %>% \n  bind_rows(rlr_final_metrics %>% mutate(model = \"Reg_LR\")) %>% \n  bind_rows(rf_final_metrics %>% mutate(model = \"RF\")) %>% \n  bind_rows(xgb_final_metrics %>% mutate(model = \"xgb\")) %>% \n  bind_rows(xgb_usemodel_final_metrics %>% mutate(model = \"xgb-usemodel\")) \n\nall_metrics_table <- all_metrics %>% \n   pivot_wider(names_from = .metric,values_from = .estimate) %>% \n   arrange(desc(accuracy))\n  \nwrite_rds(all_metrics,\"artifacts/all_metrics.rds\")\n\nall_metrics_table %>% knitr::kable(digits=3)\n```\n\n::: {.cell-output-display}\n|.estimator |.config              |model        | accuracy| roc_auc|\n|:----------|:--------------------|:------------|--------:|-------:|\n|binary     |Preprocessor1_Model1 |xgb          |    0.839|   0.870|\n|binary     |Preprocessor1_Model1 |RF           |    0.821|   0.874|\n|binary     |Preprocessor1_Model1 |Reg_LR       |    0.808|   0.842|\n|binary     |Preprocessor1_Model1 |LR           |    0.799|   0.822|\n|binary     |Preprocessor1_Model1 |NNet         |    0.786|   0.824|\n|binary     |Preprocessor1_Model1 |xgb-usemodel |    0.670|   0.797|\n:::\n:::\n\n\nand a graph:\n\n\n::: {.cell hash='index_cache/html/graph_all_metrics_c1d9090ac13576620bc5b13168610c5a'}\n\n```{.r .cell-code}\nall_metrics %>% \n  filter(.metric == \"accuracy\") %>% \n  select(model, accuracy = .estimate) %>% \n  ggplot(aes(model, accuracy)) +\n  geom_col()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/graph_all_metrics-1.png){width=672}\n:::\n:::\n\n\n# Final Submission\n\n\n::: {.cell hash='index_cache/html/predict_test_413b40f459264ff1c75f571395cc8563'}\n\n```{.r .cell-code}\n# all_predictions %>% \n# distinct(model)\n\n\n\ntest_proc <- all_proc %>% \n  filter(train_test==\"test\")\n\n# LR ----\nfinal_test_pred_LR <- \n  lr_wflow %>% \n  fit(train_proc_adj_tbl) %>% \n  predict(new_data=test_proc) %>% \n  bind_cols(test_proc)\n\nsubmission_LR <- final_test_pred_LR %>% \n  select(PassengerID = passenger_id,Survived = .pred_class)\n\nwrite_csv(submission_LR,\"titanic_submission_LR.csv\") \n\n\n# RLR ----\nfinal_test_pred_RLR <- \n  rlr_final_wflow %>% \n  fit(train_proc_adj_tbl) %>% \n  predict(new_data=test_proc) %>% \n  bind_cols(test_proc)\n\nsubmission_RLR <- final_test_pred_RLR %>% \n  select(PassengerID = passenger_id,Survived = .pred_class)\n\nwrite_csv(submission_RLR,\"titanic_submission_RLR.csv\") \n\n# RF ----\nfinal_test_pred_RF <- \n  rf_final_wflow %>% \n  fit(train_proc_adj_tbl) %>% \n  predict(new_data=test_proc) %>% \n  bind_cols(test_proc)\n\nsubmission_RF <- final_test_pred_RF %>% \n  select(PassengerID = passenger_id,Survived = .pred_class)\n\nwrite_csv(submission_RF,\"titanic_submission_RF.csv\") \n\n# NN ----\nfinal_test_pred_NN <- \n  nnet_final_wflow %>% \n  fit(train_proc_adj_tbl) %>% \n  predict(new_data=test_proc) %>% \n  bind_cols(test_proc)\n\nsubmission_NN <- final_test_pred_NN %>% \n  select(PassengerID = passenger_id,Survived = .pred_class)\n\nwrite_csv(submission_NN,\"titanic_submission_NN.csv\") \n\n\n# XGB -----\nfinal_test_pred_xgb <-\n  xgb_final_wflow %>% \n  fit(train_proc_adj_tbl) %>% \n  predict(new_data=test_proc) %>% \n  bind_cols(test_proc)\n\nsubmission_xgb <- final_test_pred_xgb %>% \n  select(PassengerID = passenger_id,Survived = .pred_class)\n\nwrite_csv(submission_xgb,\"titanic_submission_xgb.csv\")\n\n\n# ensemble -----\nfinal_test_pred_ens <-\n  ensemble %>% \n  #fit(train_proc_adj_tbl) %>% \n  predict(new_data=test_proc) %>% \n  bind_cols(test_proc)\n\nsubmission_ens <- final_test_pred_ens %>% \n  select(PassengerID = passenger_id,Survived = .pred_class)\n\nwrite_csv(submission_ens,\"titanic_submission_ens.csv\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}